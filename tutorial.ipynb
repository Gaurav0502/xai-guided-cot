{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# XAI-Guided CoT Pipeline: Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff3d9c",
   "metadata": {},
   "source": [
    "- XAI-Guided-CoT is automated pipeline for Chain-of-Thought (CoT) Prompting. In CoT, there two key types, first one in which no examples are given and second one in which few examples. We focus on the second type. In few-shot CoT, every example includes some data or information, the expected output and a reasoning. This reasoning explains how to get the expected output given the information.\n",
    "\n",
    "- In context of a tabular binary classification task, the data refers to a row of data and expected output is the class label. For human, it is very time-consuming to write this reasoning and at times, the patterns are so complex that a human cannot articulate the reasoning in words. Therefore, we design a pipeline where this reasoning can be generated automatically by an LLM from explainability attributes such as feature importances and SHAP values.\n",
    "\n",
    "- The figure below depicts the end-to-end generative-AI workflow.\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/genai_workflow.png\" alt=\"Alt text\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- In the workflow, a tree-based explainable model is trained and tuned. All the data splits and predictions are recorded for future evaluation. After training and tuning, the explainability attributes are computed: feature importances and SHAP. The SHAP CSV file is clustered to get diverse decision-making examples. The reasoning is generated for these diverse examples and they used as the reasoning component in Chain-of-Thought (CoT) prompting.\n",
    "\n",
    "<b><u>Note:</u></b> The `ObjectiveJudge` component is optional. However, `ExplanableModel`, `ReasonGenerator` and `ICLClassifier` are mandatory components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad79dd",
   "metadata": {},
   "source": [
    "Let us understand how the pipeline and its individual components can be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4373f91",
   "metadata": {},
   "source": [
    "## 1. Environmental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78542024",
   "metadata": {},
   "source": [
    "### 1.1 Create a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409964f0",
   "metadata": {},
   "source": [
    "First, create a virtual environment and install the required modules.\n",
    "\n",
    "<b><u>Note:</u></b> You should run the two commands in a terminal to create and activate a virtual environment and then choose it as the kernel for jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc24c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7d1c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/toon-format/toon-python.git (from -r requirements.txt (line 16))\n",
      "  Cloning https://github.com/toon-format/toon-python.git to /private/var/folders/29/jhvlmlq53ln75t56n34jqkrc0000gn/T/pip-req-build-a6jv368b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/toon-format/toon-python.git /private/var/folders/29/jhvlmlq53ln75t56n34jqkrc0000gn/T/pip-req-build-a6jv368b\n",
      "  Resolved https://github.com/toon-format/toon-python.git to commit 6b26984a01279defdcf40ab9d09bc418fe8133ac\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 2)) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: xgboost>=1.6.2 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: shap>=0.41.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 5)) (0.50.0)\n",
      "Requirement already satisfied: wandb>=0.15.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (0.23.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 7)) (1.2.1)\n",
      "Requirement already satisfied: jupyterlab>=3.0.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 8)) (4.5.1)\n",
      "Requirement already satisfied: ipykernel>=6.0.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 9)) (7.1.0)\n",
      "Requirement already satisfied: anthropic>=0.3.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 10)) (0.75.0)\n",
      "Requirement already satisfied: pydantic>=1.10.7 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 11)) (2.12.5)\n",
      "Requirement already satisfied: together==1.5.30 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 12)) (1.5.30)\n",
      "Requirement already satisfied: google-cloud-storage>=2.10.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 13)) (3.7.0)\n",
      "Requirement already satisfied: google-genai>=0.2.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 14)) (1.55.0)\n",
      "Requirement already satisfied: pytest>=7.0.0 in ./venv/lib/python3.13/site-packages (from -r requirements.txt (line 15)) (9.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (3.13.2)\n",
      "Requirement already satisfied: black<26.0.0,>=25.9.0 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (25.12.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (8.3.1)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (0.2.2)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (3.20.1)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (11.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (2.32.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.8.1 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (14.2.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: typer<0.20,>=0.9 in ./venv/lib/python3.13/site-packages (from together==1.5.30->-r requirements.txt (line 12)) (0.19.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic>=1.10.7->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic>=1.10.7->-r requirements.txt (line 11)) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./venv/lib/python3.13/site-packages (from pydantic>=1.10.7->-r requirements.txt (line 11)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic>=1.10.7->-r requirements.txt (line 11)) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.9.3->together==1.5.30->-r requirements.txt (line 12)) (1.22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in ./venv/lib/python3.13/site-packages (from black<26.0.0,>=25.9.0->together==1.5.30->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: packaging>=22.0 in ./venv/lib/python3.13/site-packages (from black<26.0.0,>=25.9.0->together==1.5.30->-r requirements.txt (line 12)) (25.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in ./venv/lib/python3.13/site-packages (from black<26.0.0,>=25.9.0->together==1.5.30->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in ./venv/lib/python3.13/site-packages (from black<26.0.0,>=25.9.0->together==1.5.30->-r requirements.txt (line 12)) (4.5.1)\n",
      "Requirement already satisfied: pytokens>=0.3.0 in ./venv/lib/python3.13/site-packages (from black<26.0.0,>=25.9.0->together==1.5.30->-r requirements.txt (line 12)) (0.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->together==1.5.30->-r requirements.txt (line 12)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->together==1.5.30->-r requirements.txt (line 12)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->together==1.5.30->-r requirements.txt (line 12)) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.31.0->together==1.5.30->-r requirements.txt (line 12)) (2025.11.12)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.13/site-packages (from rich<15.0.0,>=13.8.1->together==1.5.30->-r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.13/site-packages (from rich<15.0.0,>=13.8.1->together==1.5.30->-r requirements.txt (line 12)) (2.19.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.13/site-packages (from typer<0.20,>=0.9->together==1.5.30->-r requirements.txt (line 12)) (1.5.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas>=1.3.5->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas>=1.3.5->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas>=1.3.5->-r requirements.txt (line 1)) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./venv/lib/python3.13/site-packages (from scikit-learn>=1.0.2->-r requirements.txt (line 3)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./venv/lib/python3.13/site-packages (from scikit-learn>=1.0.2->-r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./venv/lib/python3.13/site-packages (from scikit-learn>=1.0.2->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in ./venv/lib/python3.13/site-packages (from shap>=0.41.0->-r requirements.txt (line 5)) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in ./venv/lib/python3.13/site-packages (from shap>=0.41.0->-r requirements.txt (line 5)) (0.63.1)\n",
      "Requirement already satisfied: cloudpickle in ./venv/lib/python3.13/site-packages (from shap>=0.41.0->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./venv/lib/python3.13/site-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (3.1.45)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./venv/lib/python3.13/site-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (6.33.2)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.13/site-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./venv/lib/python3.13/site-packages (from wandb>=0.15.0->-r requirements.txt (line 6)) (2.47.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (5.9.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (80.9.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (6.5.4)\n",
      "Requirement already satisfied: traitlets in ./venv/lib/python3.13/site-packages (from jupyterlab>=3.0.0->-r requirements.txt (line 8)) (5.14.3)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.25.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (8.7.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.23.1)\n",
      "Requirement already satisfied: pyzmq>=24 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (27.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in ./venv/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in ./venv/lib/python3.13/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (4.25.1)\n",
      "Requirement already satisfied: appnope>=0.1.2 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (1.8.19)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (9.8.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./venv/lib/python3.13/site-packages (from ipykernel>=6.0.0->-r requirements.txt (line 9)) (7.1.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from anthropic>=0.3.0->-r requirements.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in ./venv/lib/python3.13/site-packages (from anthropic>=0.3.0->-r requirements.txt (line 10)) (0.17.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from anthropic>=0.3.0->-r requirements.txt (line 10)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from anthropic>=0.3.0->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in ./venv/lib/python3.13/site-packages (from google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (2.45.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in ./venv/lib/python3.13/site-packages (from google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in ./venv/lib/python3.13/site-packages (from google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in ./venv/lib/python3.13/site-packages (from google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (2.8.0)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./venv/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (1.72.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in ./venv/lib/python3.13/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in ./venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.13/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.13/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.10.0->-r requirements.txt (line 13)) (0.6.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in ./venv/lib/python3.13/site-packages (from google-genai>=0.2.0->-r requirements.txt (line 14)) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./venv/lib/python3.13/site-packages (from google-genai>=0.2.0->-r requirements.txt (line 14)) (15.0.1)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in ./venv/lib/python3.13/site-packages (from pytest>=7.0.0->-r requirements.txt (line 15)) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./venv/lib/python3.13/site-packages (from pytest>=7.0.0->-r requirements.txt (line 15)) (1.6.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./venv/lib/python3.13/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (25.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 6)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.0->-r requirements.txt (line 6)) (5.0.2)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (3.0.52)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./venv/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.8.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2>=3.0.3->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.30.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./venv/lib/python3.13/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in ./venv/lib/python3.13/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (25.10.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together==1.5.30->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (4.14.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.13/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.13/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in ./venv/lib/python3.13/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in ./venv/lib/python3.13/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.21.2)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in ./venv/lib/python3.13/site-packages (from numba>=0.54->shap>=0.41.0->-r requirements.txt (line 5)) (0.46.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in ./venv/lib/python3.13/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel>=6.0.0->-r requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./venv/lib/python3.13/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.13/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.23)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./venv/lib/python3.13/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./venv/lib/python3.13/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.0.0->-r requirements.txt (line 8)) (1.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8980f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/toon-format/toon-python.git\n",
      "  Cloning https://github.com/toon-format/toon-python.git to /private/var/folders/29/jhvlmlq53ln75t56n34jqkrc0000gn/T/pip-req-build-yjz8xmk_\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/toon-format/toon-python.git /private/var/folders/29/jhvlmlq53ln75t56n34jqkrc0000gn/T/pip-req-build-yjz8xmk_\n",
      "  Resolved https://github.com/toon-format/toon-python.git to commit 6b26984a01279defdcf40ab9d09bc418fe8133ac\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/toon-format/toon-python.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9766f",
   "metadata": {},
   "source": [
    "### 1.2 Setup the directory structure for data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8addba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup root: /Users/gauravpendharkar/xai-guided-cot\n",
      "Created directories:\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/datasets\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/dataset_config\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/shap_values\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/batch_outputs\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/tune_config\n",
      "  - /Users/gauravpendharkar/xai-guided-cot/data/batches\n"
     ]
    }
   ],
   "source": [
    "!chmod +x setup.sh\n",
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c8494",
   "metadata": {},
   "source": [
    "This creates the directories expected by the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b270610",
   "metadata": {},
   "source": [
    "### 1.3 Getting access to the LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83cdda",
   "metadata": {},
   "source": [
    "- Now, we use three model providers in the pipeline as follows:\n",
    "\n",
    "1. Reason Generation: Together AI (https://www.together.ai/)\n",
    "2. Objective Judge: Anthropic API (https://platform.claude.com/)\n",
    "3. Classification: Google Vertex AI API (https://cloud.google.com/)\n",
    "\n",
    "- You shall go to the websites, create an account, and get an API key from Together AI and Anthropic API. \n",
    "- However, in case of Google Cloud, we use the `gcloud` CLI for authentication and not the API key approach. (Install Google Cloud SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f550565",
   "metadata": {},
   "source": [
    "- Your default browser will be opened where you would be asked to login into your Google Account. After doing that you will be redirected to https://docs.cloud.google.com/sdk/auth_success (automatically).\n",
    "\n",
    "- For the other two providers, copy your API keys and paste it inside the `.env` (created for you by the `setup.sh`) under environment variables with names `TOGETHER_API_KEY` and `CLAUDE_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948f801",
   "metadata": {},
   "source": [
    "### 1.4 Login into wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aeeb3e",
   "metadata": {},
   "source": [
    "Weights and Biases is used for the hyperparameter tuning of the tree-based explainable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9f1d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmitugaurav15\u001b[0m (\u001b[33mgauravpendharkar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128fb422",
   "metadata": {},
   "source": [
    "You can also use the API key approach. For getting the API key, you can go to https://wandb.ai/site, login with your preferred approach, and create an account. After creating your account, you will be redirected to landing page to \"track your first run\". Inside the yellow box, you can copy your API key.\n",
    "\n",
    "After getting your API key, paste it into the `.env` under the `WANDB_API_KEY` variable. This repository also needs `WANDB_PROJECT_NAME` which is included in the `.env`. Note the `WANDB_PROJECT_NAME` is not a secret but defines the environment for wandb and hence is placed inside the `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b68b5",
   "metadata": {},
   "source": [
    "### 1.5 Setting up GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fdacb",
   "metadata": {},
   "source": [
    "(Ensure you are authenticated with GCP)\n",
    "\n",
    "For the code in this repository to work as expected, you need to setup GCP as follows:\n",
    "\n",
    "1. **Install Google Cloud SDK (gcloud CLI)**\n",
    "   - If you haven't already, install the Google Cloud SDK from https://cloud.google.com/sdk/docs/install\n",
    "   - Verify installation by running: `gcloud --version`\n",
    "\n",
    "2. **Create a GCP Project**\n",
    "   - Go to https://console.cloud.google.com/\n",
    "   - Click on the project dropdown at the top and select \"New Project\"\n",
    "   - Enter a project name and note the Project ID (you'll need this later)\n",
    "   - Click \"Create\"\n",
    "\n",
    "3. **Enable Billing**\n",
    "   - Ensure billing is enabled on your account (required for Vertex AI and Cloud Storage usage)\n",
    "   - Go to Billing in the GCP Console and link a billing account to your project\n",
    "\n",
    "4. **Enable Required APIs**\n",
    "   - Enable the following APIs in your project:\n",
    "     - **Vertex AI API**: Required for batch inference jobs with Gemini models\n",
    "     - **Cloud Storage API**: Required for storing batch input files\n",
    "   - You can enable them via:\n",
    "     - GCP Console: Navigate to \"APIs & Services\" > \"Library\" and search for each API\n",
    "     - Or via command line:\n",
    "     ```bash\n",
    "       gcloud services enable aiplatform.googleapis.com\n",
    "\n",
    "       gcloud services enable storage-component.googleapis.com\n",
    "    ```\n",
    "\n",
    "5. **Create a GCS Bucket**\n",
    "   - Go to Cloud Storage in the GCP Console (https://console.cloud.google.com/storage)\n",
    "   - Click \"Create Bucket\"\n",
    "   - Choose a globally unique bucket name\n",
    "   - Select a location/region (e.g., `us-east4`, `us-central1`) - note this location as you'll need it\n",
    "   - Choose storage class and access control settings (defaults are usually fine)\n",
    "   - Click \"Create\"\n",
    "\n",
    "6. **Set Environment Variables**\n",
    "   - After setup, note the following values:\n",
    "     - **Project ID**: Found in the GCP Console project dropdown\n",
    "     - **Bucket Name**: The name you gave your GCS bucket\n",
    "     - **Location**: The region/location where your bucket was created (e.g., `us-east4`)\n",
    "   - Add these to your `.env` file as `PROJECT_ID`, `BUCKET_NAME`, and `LOCATION`.\n",
    "\n",
    "<b><u>Note:</u></b> You must use the same `LOCATION` for the GCP bucket as the location which you use to submit your batch inference. This is a constraint for this code to work properly but ideally you can use different locations.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860a993",
   "metadata": {},
   "source": [
    "Now, you have all the necessary setup to run the code in this repository! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26ddd4",
   "metadata": {},
   "source": [
    "### 1.6 Optional Unit Tests\n",
    "\n",
    "Optionally, to test if everything is working fine you run all the unit tests from `tests/` with the following command inside your terminal:\n",
    "\n",
    "```bash\n",
    "pytest -v\n",
    "\n",
    "```\n",
    "<b><u>Note:</u></b>\n",
    "- `pytest` must be executed inside the terminal\n",
    "- All tests are expected to pass successfully if all the setup is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1ceb1",
   "metadata": {},
   "source": [
    "### 1.7 Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8223eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa01956",
   "metadata": {},
   "source": [
    "## 2. Different Supported Configurations\n",
    "\n",
    "This pipeline uses three types of configuration classes to define how the pipeline operates: `Dataset`, `Model`, and `COT`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdec4cd",
   "metadata": {},
   "source": [
    "### 2.1 Dataset Configuration (`Dataset`)\n",
    "\n",
    "The `Dataset` configuration defines everything about your input data:\n",
    "\n",
    "- <b>name</b>: the name of dataset\n",
    "- <b>path</b>: the path of the original dataset CSV file. It is recommended to store all datasets inside `data/datasets` but any valid path works fine.\n",
    "- <b>config_file_path</b>: the path to the dataset configuration file. All the data splits, tree-based model predictions, and feature importances get logged in this file. It is fine if it does not exist but the directory in which it is located must exist.\n",
    "- <b>shap_vals_path</b>: the path to the CSV file to store the SHAP values for the training dataset. It is fine if it does not exist but the directory in which it is located must exist.\n",
    "- <b>preprocess_fn</b>: the preprocessing function for the dataset. It is required to ensure the dataset gets compatible with a tree-based model for training. If your dataset does not need any preprocessing, then pass an empty function that returns input dataframe itself.\n",
    "- <b>target_col</b>: the target column of the dataset.\n",
    "- <b>labels</b>: the mapping for the labels of the datasets. The length must be 2.\n",
    "\n",
    "<b>Note</b>: \n",
    "\n",
    "Pydantic is used to validate the dataset file extension, its existence, presence of a preprocessing function, and the length of the labels being two (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for dataset \n",
    "# configuration\n",
    "from scripts.configs import Dataset\n",
    "from scripts.preprocess import preprocess_titanic\n",
    "\n",
    "dataset = Dataset(\n",
    "    name=\"titanic\", # name of the dataset\n",
    "    path=\"data/datasets/titanic.csv\", # path to the csv file \n",
    "    config_file_path=\"data/dataset_config/titanic_config.json\", # path to the dataset configuration file\n",
    "    shap_vals_path=\"data/shap_values/titanic_shap.csv\", # path to the SHAP values file\n",
    "    preprocess_fn=preprocess_titanic, # preprocessing function\n",
    "    target_col=\"Survived\", # target column\n",
    "    labels={0: \"Did not survive\", 1: \"Survived\"} # labels for the target variable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2681cce",
   "metadata": {},
   "source": [
    "### 2.2 Model Configuration (`Model`)\n",
    "\n",
    "The `Model` configuration specifies which language model to use and how it should generate responses:\n",
    "\n",
    "- <b>provider</b>: the model provider. It must be `google` or `anthropic` or `together`)\n",
    "- <b>name</b>: the model name. This must be same as the string identifier inside the respective API.\n",
    "- <b>temperature</b>: the temperature of the model. This controls the determinism of the output. A lower temperature (closer to 0) is preferred for the use-cases in this code repository.\n",
    "- <b>max_tokens</b>: the maximum number of tokens. This decides how many tokens the chosen model can generate. For Gemini models, it is worth looking at the difference between thinking budget and maximum tokens.\n",
    "\n",
    "<b>Note</b>: \n",
    "\n",
    "The system validates that your provider and model combination is supported (refer: `scripts/constants`), and that `temperature >= 0` and `max_tokens > 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for \n",
    "# model configuration\n",
    "from scripts.configs import Model\n",
    "\n",
    "model = Model(\n",
    "    provider=\"anthropic\", # name of the model provider\n",
    "    name=\"claude-haiku-4-5\", # name of the model \n",
    "    temperature=0.7, # temperature for the model\n",
    "    max_tokens=1000 # maximum number of tokens the LLM can generate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b885ec",
   "metadata": {},
   "source": [
    "### 2.3 Chain-of-Thought Configuration (`COT`)\n",
    "\n",
    "The `COT` configuration controls the reasoning process used by the language model:\n",
    "\n",
    "- **Examples**: `num_examples_per_agent` - How many example cases each reasoning agent sees\n",
    "- **Reasoning Steps**: A dictionary mapping step numbers to reasoning templates or descriptions\n",
    "- **Thinking Budget**: Maximum number of reasoning steps allowed (prevents infinite loops)\n",
    "\n",
    "**Example**: You might set `num_examples_per_agent=5` to show 5 examples, define reasoning steps like `{1: \"Analyze features\", 2: \"Consider patterns\"}`, and set `thinking_budget=10` to limit reasoning to 10 steps.\n",
    "\n",
    "**Note**: The thinking budget must be â‰¥ 0 to prevent negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for \n",
    "# chain-of-thought configuration\n",
    "from scripts.configs import COT\n",
    "\n",
    "cot = COT(\n",
    "    num_examples_per_agent=5, # number of examples per agent\n",
    "    reasoning={}, # no reasoning (zero-shot cot)\n",
    "    thinking_budget=100 # thinking budget   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5adac",
   "metadata": {},
   "source": [
    "## 3. System Configuration for the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3e8dc",
   "metadata": {},
   "source": [
    "For this pipeline, the system is defined by seven configurations as follows:\n",
    "\n",
    "1. <b>a dataset</b>. Let us use the `titanic_small.csv` already available in the repository under `data/datasets`.\n",
    "2. <b>a tree-based explainable model</b>. Let us choose `XGBClassifier`.\n",
    "3. <b>a reasoning generation model</b>. Let us use the model `deepseek-ai/DeepSeek-R1` from the provider `together` with a temperature of 0.6 and maximum token limit for generation as 4096.\n",
    "4. <b>an objective judge model</b>. Let us use `claude-haiku-4-5` by `anthropic` with a temperature of 0.6 and maximum token limit for generation as 4096.\n",
    "5. <b>a chain-of-thought model</b>. Let us use `gemini-2.5-flash` by `google` with temperature of 0.0 and maximum token limit for generation as 4096.\n",
    "6. <b>a chain-of-thought configuration</b>. Fixed inside `scripts/pipeline.py`\n",
    "7. <b>a hyperparameter tuning parameter grid</b>. Let us use the sample grid `xgb.json` provided inside `data/tune_config`.\n",
    "\n",
    "<b><u>Note:</u></b> Different model providers have different syntax for getting model inferences and hence there are few limitations as follows:\n",
    "\n",
    "- The explainable must be an explainable tree-based model from the `SUPPORTED_EXPLAINABLE_MODELS` described inside `scripts/constants.py`.\n",
    "- The reasoning model can only be `deepseek-ai/DeepSeek-R1` from Together AI API.\n",
    "- The objective judge can be either `claude-sonnet-4-5` or `claude-haiku-4-5` from Anthropic API.\n",
    "- The chain-of-thought model either `gemini-2.5-flash` or `gemini-2.5-pro` from Google Vertex AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7b7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# explainable model\n",
    "explanable_model = XGBClassifier()\n",
    "\n",
    "# hyperparameter tuning \n",
    "# parameter grid\n",
    "tune_config_file = \"data/tune_config/xgb.json\"\n",
    "\n",
    "# dataset config\n",
    "dataset = Dataset(\n",
    "    name=\"titanic\", \n",
    "    path=\"data/datasets/titanic_small.csv\", \n",
    "    config_file_path=\"data/dataset_config/titanic_small_config.json\", \n",
    "    shap_vals_path=\"data/shap_values/titanic_small_shap.csv\", \n",
    "    preprocess_fn=preprocess_titanic, \n",
    "    target_col=\"Survived\", \n",
    "    labels={0: \"Did not survive\", 1: \"Survived\"}\n",
    ")\n",
    "\n",
    "# reasoning generation \n",
    "# model config\n",
    "reasoning_gen_model = Model(\n",
    "    provider=\"together\",\n",
    "    name = \"deepseek-ai/DeepSeek-R1\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# objective judge \n",
    "# model config\n",
    "objective_judge_model = Model(\n",
    "    provider=\"anthropic\",\n",
    "    name=\"claude-haiku-4-5\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# chain-of-thought \n",
    "# model config\n",
    "cot_model = Model(\n",
    "    provider=\"google\",\n",
    "    name=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0dcf3f",
   "metadata": {},
   "source": [
    "## 4. What is a Batch Inference? Why use it over a standard inference approach?\n",
    "\n",
    "Batch Inference is when multiple requests are packaged into a single request.\n",
    "\n",
    "In the context of this pipeline, the aim is to evaluate the performance of how XAI attributes can influence the Chain of Thought prompting. The focus is the classification performance and not the inference time. <b>Therefore, there is no immediate need for outputs.</b> \n",
    "\n",
    "Moreover, there are few other reasons as follows:\n",
    "\n",
    "- Every test sample needs to be processed in the isolation of the others since the reasoning for the previous test sample can serve as context for the current one. (if all or more than one test samples are put in one single prompt)\n",
    "\n",
    "- Every test sample gets one request for itself which implies a larger context window which is an asset for Chain of Thought prompting.\n",
    "\n",
    "- The cost effectiveness of batch inferences due to enhanced throughput the model providers can achieve by parallelizing the inference across different accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f5734",
   "metadata": {},
   "source": [
    "## 5. The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b91bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce verbosity\n",
    "# of logs\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_QUIET\"] = \"true\"\n",
    "os.environ[\"WANDB_CONSOLE\"] = \"off\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294a6fc",
   "metadata": {},
   "source": [
    "To use the pipeline, simply create an object of the `Pipeline` class and pass the system configurations into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde20fdc",
   "metadata": {},
   "source": [
    "### Initialize object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13307f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for pipeline\n",
    "from scripts.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    dataset=dataset, # dataset config\n",
    "    explanable_model=explanable_model, # explainable model config\n",
    "    tune_config_file=tune_config_file, # hyperparameter tuning config file path\n",
    "    reasoning_gen_model=reasoning_gen_model, # reasoning generation model config\n",
    "    objective_judge_model=objective_judge_model, # objective judge model config\n",
    "    cot_model=cot_model # chain-of-thought model config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda98f34",
   "metadata": {},
   "source": [
    "The `Pipeline.run()` has four boolean arguments that can be used to get additional results:\n",
    "\n",
    "- <b>baseline</b>: if set to true, computes the zero-shot prompting baseline.\n",
    "- <b>objective_judge</b>: if set to true, evaluates quality of the output generated by `ReasonGenerator`.\n",
    "- <b>cot_ablation</b>: if set to true, computes the zero-shot CoT performance.\n",
    "- <b>masked</b>: if set to true, bypasses the training and tuning of the explainable tree-based model because its performance is independent of the semantics of the dataset metadata.\n",
    "\n",
    "Under default setting, the pipeline performs five steps:\n",
    "\n",
    "1. Trains and tunes the tree-based explainable model (`ExplanableModel`)\n",
    "2. Extracts explanability attributes: feature importances and SHAP values.\n",
    "3. Generates natural language reasoning from numerical explainability attributes. (`ReasonGenerator`)\n",
    "4. This reasoning is passed onto the CoT model as a part of the examples to serve as references for the binary classification task.\n",
    "5. Computes standard evaluations (`accuracy` and `macro_f1_score`) for the tree-based model and XAI-Guided-CoT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a95b63",
   "metadata": {},
   "source": [
    "### Without LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4473e143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "Create sweep with ID: i6nxqdvb\n",
      "Sweep URL: https://wandb.ai/gauravpendharkar/xai-guided-cot/sweeps/i6nxqdvb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a71pcg40 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.015021334658267554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_weight: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treg_lambda: 0.01986677845807551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubsample: 0.982515323119008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XAI-MODEL] Completed hyperparameter tuning.\n",
      "[XAI-MODEL] Trained model with best hyperparameters.\n",
      "[XAI-MODEL] Logged explanation data to data/dataset_config/titanic_small_config.json\n",
      "[XAI-MODEL] Explanation process completed.\n",
      "[DIVERSE EXAMPLES] Found best number of clusters: k=10 with silhouette score: 0.47964845807183637\n",
      "[DIVERSE EXAMPLES] Chosen 10 diverse examples.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file titanic_reasoning_batches.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4k/26.4k [00:00<00:00, 47.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.COMPLETED\n",
      "[REASON GENERATION] Batch completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file titanic_reasoning_predictions.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64.2k/64.2k [00:00<00:00, 19.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Batch outputs downloaded to data/batch_outputs/titanic_reasoning_predictions.jsonl\n",
      "[PIPELINE] Reasoning generation completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_icl_batches.jsonl uploaded to batch_inputs/gemini/titanic_icl_batches.jsonl\n",
      "[ICL CLASSIFIER] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424\n",
      "[ICL CLASSIFIER] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini/titanic_cot_1765888259/\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_QUEUED\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2588022497999847424 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ICL CLASSIFIER] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_cot_1765888259/prediction-model-2025-12-16T12:31:00.347901Z/predictions.jsonl to data/batch_outputs/titanic_cot_predictions.jsonl.\n",
      "[PIPELINE] ICL classification with COT completed.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[PIPELINE] Evaluation of COT predictions completed.\n",
      "[PIPELINE] Pipeline run completed.\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebf3ce",
   "metadata": {},
   "source": [
    "### With LLM-as-a-Judge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b72ec335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "Create sweep with ID: mwsbp9ts\n",
      "Sweep URL: https://wandb.ai/gauravpendharkar/xai-guided-cot/sweeps/mwsbp9ts\n",
      "[XAI-MODEL] Completed hyperparameter tuning.\n",
      "[XAI-MODEL] Trained model with best hyperparameters.\n",
      "[XAI-MODEL] Logged explanation data to data/dataset_config/titanic_small_config.json\n",
      "[XAI-MODEL] Explanation process completed.\n",
      "[DIVERSE EXAMPLES] Found best number of clusters: k=10 with silhouette score: 0.32333119714604575\n",
      "[DIVERSE EXAMPLES] Chosen 10 diverse examples.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file titanic_reasoning_batches.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.6k/26.6k [00:00<00:00, 49.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.COMPLETED\n",
      "[REASON GENERATION] Batch completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file titanic_reasoning_predictions.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74.3k/74.3k [00:00<00:00, 16.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Batch outputs downloaded to data/batch_outputs/titanic_reasoning_predictions.jsonl\n",
      "[PIPELINE] Reasoning generation completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[OBJECTIVE JUDGE] Submitted batch with id: msgbatch_01DuSTJojjmihTjefqjF3UNi\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01DuSTJojjmihTjefqjF3UNi is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01DuSTJojjmihTjefqjF3UNi has completed processing.\n",
      "[OBJECTIVE JUDGE] Batch result types: {'succeeded': 10, 'errored': 0, 'expired': 0}\n",
      "[OBJECTIVE JUDGE] Saved evaluations to data/batch_outputs/titanic_objective_judge_evaluations.jsonl\n",
      "[PIPELINE] Objective judge evaluation completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_icl_batches.jsonl uploaded to batch_inputs/gemini/titanic_icl_batches.jsonl\n",
      "[ICL CLASSIFIER] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592\n",
      "[ICL CLASSIFIER] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini/titanic_cot_1765889227/\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/3946983685558894592 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ICL CLASSIFIER] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_cot_1765889227/prediction-model-2025-12-16T12:47:08.145979Z/predictions.jsonl to data/batch_outputs/titanic_cot_predictions.jsonl.\n",
      "[PIPELINE] ICL classification with COT completed.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[PIPELINE] Evaluation of COT predictions completed.\n",
      "[PIPELINE] Pipeline run completed.\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(objective_judge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730a8e7",
   "metadata": {},
   "source": [
    "- In few cases, batch inferences can fail, expire, or cancel. In such cases, it must be important to know how to use the individual components. With reference to this pipeline, the `ReasonGenerator` and `ICLClassifier` are mandatory components. In either case, errors will be raised.\n",
    "\n",
    "- Considering the fact that pydantic is used to validate the range of hyperparameters and the model names, it is unlikely that the batch will fail because of invalid input. It might expire or cancel due to issues from the model provider but that is very rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892ce79",
   "metadata": {},
   "source": [
    "## 6. Individual Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d1525",
   "metadata": {},
   "source": [
    "While using the individual components, there are three steps:\n",
    "\n",
    "1. <b>Initialization of the component</b>\n",
    "\n",
    "This step is just creating an object of the respective class with the correct system configuration described in previous sections.\n",
    "\n",
    "2. <b>Running the inference</b>\n",
    "\n",
    "This step includes many sub-steps as follows:\n",
    "\n",
    "- First the all the batches for test examples are created and structured as a JSONL file. JSONL file is a file in which every line is a JSON document.\n",
    "\n",
    "- This JSONL is uploaded to the model provider's cloud storage (e.g. Google Cloud Bucket for Vertex AI Batch Inference)\n",
    "\n",
    "- The batch inference job is submitted to the model provider. This might take as much as 24 hours to get into execution. However, based off batch inferences done during debugging and testing, the batch inference starts running within couple of seconds.\n",
    "\n",
    "- We synchronously monitor it and automatically download the JSONL result file to local storage. This file contains the output for each request inside the input JSONL file whether the request succeeds or fails. However, the order of the output does not match with that of input file and hence, a meaningful `request_id` is pivotal to mapping the results for futher evaluation.\n",
    "\n",
    "3. <b>Parsing the result</b>\n",
    "\n",
    "This step just parses the output text generated by the LLM. However, the catch is the error handling because the the output of the LLM is non-deterministic and it may not follow the given output format always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1987bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the results\n",
    "# properly\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab21ac",
   "metadata": {},
   "source": [
    "### 6.1 ReasonGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe69111",
   "metadata": {},
   "source": [
    "This component converts numerical explainability attributes into natural language reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d40df",
   "metadata": {},
   "source": [
    "#### 6.1.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5244612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for reasoning generation\n",
    "from scripts.reason_generation import ReasonGenerator\n",
    "from scripts.prompt_generator import reasoning_prompt_generator\n",
    "\n",
    "reason_generator = ReasonGenerator(\n",
    "    dataset=dataset, # dataset config\n",
    "    model=reasoning_gen_model, # reasoning generation model config\n",
    "    prompt_gen_fn=reasoning_prompt_generator # reasoning prompt generator function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c0725",
   "metadata": {},
   "source": [
    "#### 6.1.2 Running the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8856a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIVERSE EXAMPLES] Found best number of clusters: k=10 with silhouette score: 0.32333119714604575\n",
      "[DIVERSE EXAMPLES] Chosen 10 diverse examples.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file titanic_reasoning_batches.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.6k/26.6k [00:00<00:00, 50.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.COMPLETED\n",
      "[REASON GENERATION] Batch completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file titanic_reasoning_predictions.jsonl: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67.9k/67.9k [00:00<00:00, 7.02MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Batch outputs downloaded to data/batch_outputs/titanic_reasoning_predictions.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reason_generator.create_batch_prompts() # create batch prompts\n",
    "reason_generator.save_batches_as_jsonl() # save batches as jsonl (locally)\n",
    "reason_generator.submit_batches() # submit batches (to the model provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60374e7",
   "metadata": {},
   "source": [
    "As explained above, the SHAP values CSV file is clustered first and there seem to be only 10 unique decision-making patterns. The dataset is preprocessed to ensure that the row indices for the diverse examples point towards the correct data points. After that, the `titanic_reasoning_batches.jsonl` file uploaded to the cloud storage on Together AI. The batch inference job is submitted and instantly starts processing. Finally, the batch completes successfully and the `titanic_reasoning_predictions.jsonl` file is downloaded from Together AI's cloud storage and saved locally to `data/batch_outputs/titanic_reasoning_predictions.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f453076",
   "metadata": {},
   "source": [
    "#### 6.1.3 Parsing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for parsing the results\n",
    "from scripts.postprocess import parse_reasoning_llm_results\n",
    "\n",
    "reasoning = parse_reasoning_llm_results(\n",
    "    results_jsonl_path=reason_generator.destination_file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baab6c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{104: 'The model correctly predicted that the passenger did not survive (0), '\n",
      "      'matching the ground truth label (0.0). This outcome is primarily driven '\n",
      "      'by the strong negative contributions of the two most important '\n",
      "      \"features: Sex (SHAP = -1.15) and Pclass (SHAP = -0.99). The passenger's \"\n",
      "      'male gender (Sex=0.0) and third-class ticket (Pclass=3.0) substantially '\n",
      "      'decrease survival probability, as these are the highest-impact features '\n",
      "      'in the model (overall importances of 0.44 and 0.19 respectively). While '\n",
      "      \"the passenger's age (30.5 years) provides a modest positive \"\n",
      "      'contribution (SHAP=0.14), it is outweighed by the dominant negative '\n",
      "      'factors. Other features like Fare (8.05, SHAP=-0.02) and SibSp/Parch '\n",
      "      '(both 0.0, SHAPâ‰ˆ0) have negligible impacts. The cumulative effect of '\n",
      "      'these SHAP valuesâ€”especially the large negative contributions from Sex '\n",
      "      'and Pclassâ€”clearly pushes the prediction below the decision threshold '\n",
      "      'for survival.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(list(reasoning.items())[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfdbc3a",
   "metadata": {},
   "source": [
    "In the reasoning output, the dictionary is organized in such a way that the key is the index of the row in the original dataset after preprocessing and the value is the natural language reasoning generated by `ReasonGenerator`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffd889",
   "metadata": {},
   "source": [
    "### 6.2 ObjectiveJudge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d020f",
   "metadata": {},
   "source": [
    "This component evaluates the natural language reasoning generated by the `ReasonGenerator`. We use the LLM-as-a-Judge approach for evaluation since there are no manually annotated reasoning outputs to compare the generations and compute metrics like ROUGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3b597",
   "metadata": {},
   "source": [
    "#### 6.2.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for objective judge\n",
    "from scripts.objective_judge import ObjectiveJudge\n",
    "from scripts.prompt_generator import objective_judge_prompt_generator\n",
    "\n",
    "judge = ObjectiveJudge(\n",
    "    dataset=dataset, # dataset config\n",
    "    model=objective_judge_model, # objective judge model config\n",
    "    prompt_gen_fn=objective_judge_prompt_generator # objective judge prompt generator function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938837b",
   "metadata": {},
   "source": [
    "#### 6.2.2 Running the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "355e92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[OBJECTIVE JUDGE] Submitted batch with id: msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_01Jf6sMCTJVo3KwKiaSWQ7Jo has completed processing.\n",
      "[OBJECTIVE JUDGE] Batch result types: {'succeeded': 10, 'errored': 0, 'expired': 0}\n",
      "[OBJECTIVE JUDGE] Saved evaluations to data/batch_outputs/titanic_objective_judge_evaluations.jsonl\n"
     ]
    }
   ],
   "source": [
    "judge.create_batch_prompts(reasoning=reasoning) # reasoning for the examples (generated by ReasonGenerator)\n",
    "judge.submit_batch() # submit the batch inference job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25382f82",
   "metadata": {},
   "source": [
    "The dataset is preprocessed again so that reasoning output from `ReasonGenerator` can be mapped to the correct rows in the dataset. The batch inference job gets submitted to Anthropic API and it instantly starts processing. All 10 requests are successful and evaluations downloaded from Anthropic's cloud storage to `data/batch_outputs/titanic_objective_judge_evaluations.jsonl` on the local system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511a8ab",
   "metadata": {},
   "source": [
    "#### 6.2.3 Parsing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for parsing the results\n",
    "from scripts.postprocess import parse_objective_judge_results\n",
    "\n",
    "reasoning_eval = parse_objective_judge_results(\n",
    "    results_jsonl_path=judge.destination_file_name # path to the jsonl file for objective judge evaluations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14881941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 31: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 41: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 77: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 91: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 104: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 109: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 126: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 172: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75},\n",
      " 176: {'coherence': 4.75, 'consistency': 4.75, 'faithfulness': 4.75}}\n"
     ]
    }
   ],
   "source": [
    "pprint(reasoning_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e1f6f",
   "metadata": {},
   "source": [
    "Here, the output is a dictionary with the key as the row index of the dataset after preprocessing and the value is a dictionary with the metrics as key and the score as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607aa881",
   "metadata": {},
   "source": [
    "### 6.3 ICLClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db9b4d",
   "metadata": {},
   "source": [
    "This component predicts a class label for a given test sample by using the given reasoning examples. The natural language reasoning used in this component is generated by the  `ReasonGenerator`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3db21",
   "metadata": {},
   "source": [
    "#### 6.3.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85095657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module used for ICL classification\n",
    "from scripts.icl_classification import ICLClassifier\n",
    "from scripts.prompt_generator import cot_prompt_generator\n",
    "\n",
    "cot_config = COT(\n",
    "    num_examples_per_agent=10, # number of examples every agent sees\n",
    "    reasoning=reasoning, # reasoning for the examples (generated by ReasonGenerator)\n",
    "    thinking_budget=1000 # maximum number of tokens for internal thinking\n",
    ")\n",
    "\n",
    "icl_classifier = ICLClassifier(\n",
    "    dataset=dataset, # dataset config\n",
    "    model=cot_model, # chain-of-thought model config\n",
    "    cot=cot_config, # chain-of-thought configuration\n",
    "    prompt_gen_fn=cot_prompt_generator # chain-of-thought prompt generator function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cbd2d",
   "metadata": {},
   "source": [
    "#### 6.3.2 Running the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71460441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_icl_batches.jsonl uploaded to batch_inputs/gemini/titanic_icl_batches.jsonl\n",
      "[ICL CLASSIFIER] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936\n",
      "[ICL CLASSIFIER] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini/titanic_cot_1765890383/\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/2433774210762407936 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ICL CLASSIFIER] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_cot_1765890383/prediction-model-2025-12-16T13:06:24.541997Z/predictions.jsonl to data/batch_outputs/titanic_cot_predictions.jsonl.\n"
     ]
    }
   ],
   "source": [
    "icl_classifier.create_batch_prompts() # create batch prompts\n",
    "icl_classifier.save_batches_as_jsonl() # save batches as jsonl (locally)\n",
    "icl_classifier.upload_batches_to_gcs() # upload batches to gcp bucket\n",
    "icl_classifier.submit_batch_inference_job() # submit batch inference job (vertex ai)\n",
    "icl_classifier.download_job_outputs_from_gcs() # download job outputs from gcp bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ad652",
   "metadata": {},
   "source": [
    "The dataset is preprocessed to map the natural language reasoning with correct rows in the dataset. The locally saved batch inputs JSONL file `data/batches/titanic_icl_batches.jsonl` is uploaded to the GCP bucket at `batch_inputs/gemini/titanic_icl_batches.jsonl`. The job is submitted to Vertex AI and it starts running instantly. After the job succeeds, the output is downloaded automatically from the GCP bucket to the local system at `data/batch_outputs/titanic_cot_predictions.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3dc2e",
   "metadata": {},
   "source": [
    "#### 6.3.3 Parsing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0830b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# module used for parsing the results\n",
    "from scripts.postprocess import parse_cot_llm_results\n",
    "\n",
    "predictions = parse_cot_llm_results(\n",
    "    results_jsonl_path=icl_classifier.destination_file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55f437e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15: 1, 62: 1, 117: 0, 118: 1, 120: 0}\n"
     ]
    }
   ],
   "source": [
    "pprint(dict(list(predictions.items())[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7170d73",
   "metadata": {},
   "source": [
    "The output is a dictionary with the key as the row index of the dataset after preprocessing and the value is the predicted class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55a06a",
   "metadata": {},
   "source": [
    "This tutorial is about how to use the pipeline components shown inside the GenAI workflow. The `ZeroShotBaseline` is only used to establish a baseline performance to compare it with `XAI-Guided-CoT`. If you want to use this component, then the experimental results are right place. From functionality standpoint, the `ZeroShotBaseline` uses the same LLM configuration as that of `ICLClassifier`. The only difference is in the prompt and `thinkingBudget` hyperparameter. The two files could be combined into a single class but are kept as separate for better interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabb6b7",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "This guide addresses common issues you might encounter while using the code in this repository. Issues are organized by category for easier navigation.\n",
    "\n",
    "### 1. Installation and Dependencies\n",
    "\n",
    "**Problem: Dependency conflicts or installation errors**\n",
    "\n",
    "- **Solution**: Always use a virtual environment to isolate your project dependencies:\n",
    "\n",
    "```bash\n",
    "  python3 -m venv venv\n",
    "  source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "  pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "  - **Verification**: Run `pytest -v` from the `tests/` directory to verify all dependencies are correctly installed.\n",
    "\n",
    "**Problem: Missing `toon-python` package**\n",
    "\n",
    "- **Solution**: Install it separately:h\n",
    "  pip install git+https://github.com/toon-format/toon-python.git\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Environment Variables and Authentication\n",
    "\n",
    "**Problem: Missing or incorrect environment variables**\n",
    "\n",
    "- **Symptoms**: `KeyError` when accessing environment variables, `None` values, or authentication failures\n",
    "- **Solution**: \n",
    "  - Verify your `.env` file contains all required variables:\n",
    "    - `PROJECT_ID`, `BUCKET_NAME`, `LOCATION` (GCP)\n",
    "    - `TOGETHER_API_KEY`, `CLAUDE_API_KEY` (LLM APIs)\n",
    "    - `WANDB_API_KEY`, `WANDB_PROJECT_NAME` (Weights & Biases)\n",
    "  - Ensure no extra spaces or quotes around values\n",
    "  - Restart your Python kernel/terminal after modifying `.env`\n",
    "\n",
    "**Problem: GCP authentication failures**\n",
    "\n",
    "- **Symptoms**: `google.auth.exceptions.DefaultCredentialsError` or upload failures\n",
    "- **Solution**:\n",
    "  - Re-run: `gcloud auth application-default login`\n",
    "  - Verify billing is enabled on your GCP project\n",
    "  - Ensure Vertex AI API and Cloud Storage API are enabled\n",
    "  - Check that `PROJECT_ID` matches your GCP project\n",
    "\n",
    "**Problem: API key authentication failures**\n",
    "\n",
    "- **Symptoms**: 401/403 errors from Together AI or Anthropic\n",
    "- **Solution**:\n",
    "  - Verify API keys are correct and not expired\n",
    "  - Check API quotas/limits haven't been exceeded\n",
    "  - Ensure keys are in `.env` without quotes\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Configuration Errors\n",
    "\n",
    "**Problem: Invalid model/provider combination**\n",
    "\n",
    "- **Symptoms**: `ValueError: Invalid model name: X for provider: Y`\n",
    "- **Solution**: \n",
    "  - Check `scripts/constants.py` for supported combinations:\n",
    "    - Reasoning model: `deepseek-ai/DeepSeek-R1` (Together AI only)\n",
    "    - Objective judge: `claude-sonnet-4-5` or `claude-haiku-4-5` (Anthropic only)\n",
    "    - CoT model: `gemini-2.5-flash` or `gemini-2.5-pro` (Google only)\n",
    "  - To use other models, update `VALID_PROVIDERS` and `VALID_MODELS` in `scripts/constants.py`\n",
    "\n",
    "**Problem: Dataset configuration errors**\n",
    "\n",
    "- **Symptoms**: `FileNotFoundError`, `TypeError`, or `ValueError` during Dataset initialization\n",
    "- **Solution**:\n",
    "  - Verify CSV file exists and path is correct (relative or absolute)\n",
    "  - Ensure `preprocess_fn` is a callable function (not a string)\n",
    "  - Verify `labels` dictionary has exactly 2 entries: `{0: \"Class 0\", 1: \"Class 1\"}`\n",
    "  - Check that `target_col` exists in your dataset\n",
    "  - Refer to `scripts/configs.py` for validation rules\n",
    "\n",
    "**Problem: Invalid hyperparameters**\n",
    "\n",
    "- **Symptoms**: `ValueError: Temperature must be >= 0` or `max_tokens must be > 0`\n",
    "- **Solution**:\n",
    "  - Set `temperature >= 0` (typically 0.0-1.0)\n",
    "  - Set `max_tokens > 0` (check model-specific limits)\n",
    "  - For Gemini models: remember `maxOutputTokens = thinkingBudget + externalTokens`\n",
    "\n",
    "---\n",
    "\n",
    "### 4. GCP and Cloud Storage Issues\n",
    "\n",
    "**Problem: Bucket upload failures**\n",
    "\n",
    "- **Symptoms**: `AssertionError: ICL Classifier is a mandatory component. Kindly debug the issue`\n",
    "- **Solution**:\n",
    "  - Verify bucket name exists and is correct\n",
    "  - Check GCP permissions (Storage Admin role)\n",
    "  - Ensure bucket `LOCATION` matches environment variable\n",
    "  - Verify `gcloud` authentication: `gcloud auth application-default login`\n",
    "\n",
    "**Problem: Vertex AI batch job submission failures**\n",
    "\n",
    "- **Symptoms**: `AssertionError: Error submitting batch job to Vertex AI`\n",
    "- **Solution**:\n",
    "  - Ensure Vertex AI API is enabled: `gcloud services enable aiplatform.googleapis.com`\n",
    "  - Verify `PROJECT_ID` and `LOCATION` are correct\n",
    "  - Ensure `LOCATION` matches your bucket region\n",
    "  - Check billing is enabled\n",
    "\n",
    "**Problem: Missing predictions.jsonl file**\n",
    "\n",
    "- **Symptoms**: `ValueError: No predictions.jsonl file found in GCS location`\n",
    "- **Solution**:\n",
    "  - Wait for batch job to complete (check job status in GCP Console)\n",
    "  - Verify batch job succeeded (not failed/cancelled)\n",
    "  - Check GCS bucket path matches expected output directory\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Data and Preprocessing Issues\n",
    "\n",
    "**Problem: SHAP values CSV missing 'idx' column**\n",
    "\n",
    "- **Symptoms**: `KeyError: SHAP values CSV must contain an 'idx' column`\n",
    "- **Solution**:\n",
    "  - Re-run the `ExplainableModel` step to regenerate SHAP values\n",
    "  - Ensure preprocessing function maintains consistent row indices\n",
    "  - Verify SHAP CSV structure matches expected format\n",
    "\n",
    "**Problem: Empty reasoning outputs**\n",
    "\n",
    "- **Symptoms**: `AssertionError: Reasoning outputs are required`\n",
    "- **Solution**:\n",
    "  - Check raw LLM outputs in `data/batch_outputs/titanic_reasoning_predictions.jsonl`\n",
    "  - Verify LLM followed expected output format\n",
    "  - May need to adjust prompt or model temperature\n",
    "  - Re-run pipeline or re-submit batch using `data/batches/titanic_reasoning_batches.jsonl`\n",
    "\n",
    "**Problem: Preprocessing errors**\n",
    "\n",
    "- **Symptoms**: Errors during dataset loading or preprocessing\n",
    "- **Solution**:\n",
    "  - Ensure preprocessing function handles missing values correctly\n",
    "  - Verify all required columns exist in dataset\n",
    "  - Check data types are compatible with tree-based models\n",
    "  - Ensure target column has binary values (0 and 1)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Batch Inference Issues\n",
    "\n",
    "**Problem: Batch jobs failing, expiring, or cancelling**\n",
    "\n",
    "- **Symptoms**: Job status shows `FAILED`, `EXPIRED`, or `CANCELLED`\n",
    "- **Solution**:\n",
    "  - Check GCP Console (for Vertex AI) or provider dashboard for detailed error messages\n",
    "  - Verify input JSONL format is valid\n",
    "  - Check for token limit issues (increase `max_tokens`)\n",
    "  - Ensure API quotas/limits aren't exceeded\n",
    "  - Retry the batch submission\n",
    "\n",
    "**Problem: Token limit interruptions**\n",
    "\n",
    "- **Symptoms**: `[POSTPROCESS] X requests were interrupted due to token limit`\n",
    "- **Solution**:\n",
    "  - Increase `max_tokens` in model configuration\n",
    "  - Reduce `num_examples_per_agent` in COT config\n",
    "  - Simplify prompts or reduce reasoning complexity\n",
    "  - For Gemini: adjust `thinkingBudget` relative to `max_tokens`\n",
    "\n",
    "**Problem: LLM output format mismatches**\n",
    "\n",
    "- **Symptoms**: Parsing errors, missing predictions, or empty reasoning outputs\n",
    "- **Solution**:\n",
    "  - Check raw outputs in JSONL files (`data/batch_outputs/`)\n",
    "  - Verify LLM follows expected format (check prompt templates)\n",
    "  - Adjust prompts to be more explicit about output format\n",
    "  - Consider using lower temperature for more deterministic outputs\n",
    "  - **Note**: DeepSeek-R1 has been observed to violate output formats more frequently than Gemini/Anthropic models\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Wandb and Hyperparameter Tuning Issues\n",
    "\n",
    "**Problem: Wandb sweep creation failures**\n",
    "\n",
    "- **Symptoms**: Errors when creating wandb sweep\n",
    "- **Solution**:\n",
    "  - Verify `WANDB_API_KEY` and `WANDB_PROJECT_NAME` are set in `.env`\n",
    "  - Run `wandb.login()` if not already authenticated\n",
    "  - Check sweep configuration JSON file is valid\n",
    "  - Ensure wandb project exists or can be created\n",
    "\n",
    "**Problem: Hyperparameter tuning errors**\n",
    "\n",
    "- **Symptoms**: Errors during model training in sweep\n",
    "- **Solution**:\n",
    "  - Verify hyperparameter ranges in config file are valid\n",
    "  - Check dataset is properly preprocessed\n",
    "  - Ensure sufficient training data\n",
    "  - Review wandb logs for specific error messages\n",
    "\n",
    "**Problem: Wandb config parsing issues (older versions)**\n",
    "\n",
    "- **Symptoms**: `best_run.config` returns text instead of dict\n",
    "- **Solution**: \n",
    "  - Update to latest `wandb` version: `pip install --upgrade wandb`\n",
    "  - If using older version, parse manually: `json.loads(best_run.config)`\n",
    "\n",
    "---\n",
    "\n",
    "### 8. General Debugging Tips\n",
    "\n",
    "**Check intermediate outputs:**\n",
    "- Inspect JSONL files in `data/batches/` and `data/batch_outputs/`\n",
    "- Verify batch files are properly formatted\n",
    "- Check for malformed JSON or missing fields\n",
    "\n",
    "**Verify component initialization:**\n",
    "- Test individual components separately before running full pipeline\n",
    "- Use `masked=True` in `pipeline.run()` to skip model training if debugging other components\n",
    "- Check component attributes after initialization\n",
    "\n",
    "**Monitor batch job status:**\n",
    "- Check GCP Console for Vertex AI batch job status\n",
    "- Monitor Together AI dashboard for batch job progress\n",
    "- Check Anthropic API for batch processing status\n",
    "\n",
    "**Common fixes:**\n",
    "- Restart Python kernel after changing `.env` file\n",
    "- Re-authenticate with `gcloud auth application-default login`\n",
    "- Clear cached files and re-run preprocessing\n",
    "- Verify all directory paths exist before running pipeline\n",
    "- Run unit tests (`pytest -v`) after customizing code\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Getting Additional Help\n",
    "\n",
    "If you encounter issues not covered here:\n",
    "\n",
    "1. **Check the code**: Review `scripts/configs.py` and `scripts/constants.py` for validation rules\n",
    "2. **Run unit tests**: Execute `pytest -v` to verify basic functionality\n",
    "3. **Inspect logs**: Check console output and error messages for specific details\n",
    "4. **Review intermediate files**: Examine JSONL files in `data/batches/` and `data/batch_outputs/`\n",
    "5. **GitHub Issues**: Feel free to raise an issue on GitHub with:\n",
    "   - Error message and traceback\n",
    "   - Your configuration (without sensitive keys)\n",
    "   - Steps to reproduce\n",
    "   - Relevant log files or intermediate outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
