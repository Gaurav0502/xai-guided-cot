{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# XAI-Guided CoT Pipeline Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c26708",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Please run the **four** commands below in the terminal to create a virtual environment and download the necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n yourenv python=3.11.14\n",
    "# conda activate yourenv\n",
    "# pip install -r requirements.txt\n",
    "# pip install git+https://github.com/toon-format/toon-python.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d683a5a",
   "metadata": {},
   "source": [
    "## Environment variables (.env)\n",
    "\n",
    "Create a `.env` file in the project root and add the following variables (use your own values):\n",
    "\n",
    "```dotenv\n",
    "# --- Weights & Biases ---\n",
    "WANDB_API_KEY=...\n",
    "WANDB_PROJECT_NAME=...\n",
    "\n",
    "# --- Your project naming ---\n",
    "PROJECT_NAME=...\n",
    "\n",
    "# --- Google Cloud ---\n",
    "PROJECT_ID=...\n",
    "BUCKET_NAME=...\n",
    "LOCATION=...\n",
    "\n",
    "# --- LLM Providers ---\n",
    "TOGETHER_API_KEY=...\n",
    "CLAUDE_API_KEY=...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, json, time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from scripts.pipeline import Pipeline\n",
    "from scripts.configs import Dataset, Model\n",
    "from scripts.preprocess import (\n",
    "    preprocess_titanic, preprocess_mushroom, \n",
    "    preprocess_diabetes, preprocess_loan\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "WANDB_PROJECT_NAME = os.getenv(\"WANDB_PROJECT_NAME\")\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0f9e0",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Please download the datasets from the links below and place the **CSV files** in the `data/datasets/` directory.\n",
    "\n",
    "- **(a) Titanic dataset:** https://www.kaggle.com/datasets/yasserh/titanic-dataset  \n",
    "- **(b) Pima Diabetes:** https://www.kaggle.com/datasets/jamaltariqcheema/pima-indians-diabetes-dataset  \n",
    "- **(c) Loan Approval:** https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset  \n",
    "- **(d) Mushroom:** https://www.kaggle.com/datasets/uciml/mushroom-classification  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "datasets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: ['titanic', 'diabetes', 'loan', 'mushroom']\n",
      "Masked datasets: ['titanic', 'diabetes', 'loan', 'mushroom']\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "datasets = {\n",
    "    \"titanic\": Dataset(\n",
    "        name=\"titanic\", path=\"data/datasets/titanic.csv\",\n",
    "        config_file_path=\"data/dataset_config/titanic_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/titanic_shap.csv\",\n",
    "        preprocess_fn=preprocess_titanic, target_col=\"Survived\",\n",
    "        labels={0: \"Did not survive\", 1: \"Survived\"}\n",
    "    ),\n",
    "    \"diabetes\": Dataset(\n",
    "        name=\"diabetes\", path=\"data/datasets/diabetes.csv\",\n",
    "        config_file_path=\"data/dataset_config/diabetes_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/diabetes_shap.csv\",\n",
    "        preprocess_fn=preprocess_diabetes, target_col=\"Outcome\",\n",
    "        labels={0: \"No Diabetes\", 1: \"Has Diabetes\"}\n",
    "    ),\n",
    "    \"loan\": Dataset(\n",
    "        name=\"loan\", path=\"data/datasets/loan_approval.csv\",\n",
    "        config_file_path=\"data/dataset_config/loan_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/loan_shap.csv\",\n",
    "        preprocess_fn=preprocess_loan, target_col=\"loan_status\",\n",
    "        labels={0: \"Rejected\", 1: \"Approved\"}\n",
    "    ),\n",
    "    \"mushroom\": Dataset(\n",
    "        name=\"mushroom\", path=\"data/datasets/mushrooms.csv\",\n",
    "        config_file_path=\"data/dataset_config/mushroom_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/mushroom_shap.csv\",\n",
    "        preprocess_fn=preprocess_mushroom, target_col=\"class\",\n",
    "        labels={0: \"Edible\", 1: \"Poisonous\"}\n",
    "    )\n",
    "}\n",
    "\n",
    "# [TO DO] Update for the masked datasets\n",
    "# Masked versions \n",
    "datasets_masked = {\n",
    "    \"titanic\": Dataset(\n",
    "        name=\"titanic_masked\", path=\"data/datasets/titanic_masked.csv\",\n",
    "        config_file_path=\"data/dataset_config/titanic_masked_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/titanic_masked_shap.csv\",\n",
    "        preprocess_fn=preprocess_titanic, target_col=\"Survived\",\n",
    "        labels={0: \"0\", 1: \"1\"}\n",
    "    ),\n",
    "    \"diabetes\": Dataset(\n",
    "        name=\"diabetes_masked\", path=\"data/datasets/diabetes_masked.csv\",\n",
    "        config_file_path=\"data/dataset_config/diabetes_masked_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/diabetes_masked_shap.csv\",\n",
    "        preprocess_fn=preprocess_diabetes, target_col=\"Outcome\",\n",
    "        labels={0: \"0\", 1: \"1\"}\n",
    "    ),\n",
    "    \"loan\": Dataset(\n",
    "        name=\"loan_masked\", path=\"data/datasets/loan_masked.csv\",\n",
    "        config_file_path=\"data/dataset_config/loan_masked_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/loan_masked_shap.csv\",\n",
    "        preprocess_fn=preprocess_loan, target_col=\"loan_status\",\n",
    "        labels={0: \"0\", 1: \"1\"}\n",
    "    ),\n",
    "    \"mushroom\": Dataset(\n",
    "        name=\"mushroom_masked\", path=\"data/datasets/mushroom_masked.csv\",\n",
    "        config_file_path=\"data/dataset_config/mushroom_masked_config.json\",\n",
    "        shap_vals_path=\"data/shap_values/mushroom_masked_shap.csv\",\n",
    "        preprocess_fn=preprocess_mushroom, target_col=\"class\",\n",
    "        labels={0: \"0\", 1: \"1\"}\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Datasets: {list(datasets.keys())}\")\n",
    "print(f\"Masked datasets: {list(datasets_masked.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured.\n"
     ]
    }
   ],
   "source": [
    "# Model Configurations\n",
    "reasoning_gen_model = Model(\n",
    "    provider=\"together\", name=\"deepseek-ai/DeepSeek-R1\",\n",
    "    temperature=0.6, max_tokens=4096\n",
    ")\n",
    "objective_judge_model = Model(\n",
    "    provider=\"anthropic\", name=\"claude-haiku-4-5\",\n",
    "    temperature=0.6, max_tokens=4096\n",
    ")\n",
    "cot_model = Model(\n",
    "    provider=\"google\", name=\"gemini-2.5-flash\",\n",
    "    temperature=0.0, max_tokens=4096\n",
    ")\n",
    "\n",
    "TUNE_CONFIG_FILE = \"data/tune_config/xgb.json\"\n",
    "print(\"Models configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def run_pipeline(\n",
    "    dataset: Dataset,\n",
    "    output_dir: str,\n",
    "    masked: bool = False,\n",
    "    baseline: bool = True,\n",
    "    objective_judge: bool = True,\n",
    "    cot_ablation: bool = True\n",
    ") -> dict:\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    suffix = \"masked\" if masked else \"unmasked\"\n",
    "    filename = f\"{dataset.name}_{suffix}_{int(time.time())}.json\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {dataset.name} ({suffix})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        dataset=dataset,\n",
    "        explanable_model=XGBClassifier(),\n",
    "        tune_config_file=TUNE_CONFIG_FILE,\n",
    "        reasoning_gen_model=reasoning_gen_model,\n",
    "        objective_judge_model=objective_judge_model,\n",
    "        cot_model=cot_model\n",
    "    )\n",
    "    \n",
    "    pipeline.run(\n",
    "        baseline=baseline,\n",
    "        objective_judge=objective_judge,\n",
    "        cot_ablation=cot_ablation,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results = {\n",
    "        \"dataset\": dataset.name,\n",
    "        \"masked\": masked,\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "        \"metrics\": {k: v for k, v in pipeline.results.items() if k != \"reasoning\"}\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Completed in {elapsed:.2f}s. Saved to {output_path}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running: titanic (unmasked)\n",
      "============================================================\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "Create sweep with ID: 2nptjmtl\n",
      "Sweep URL: https://wandb.ai/hl3925-columbia-university/6998final/sweeps/2nptjmtl\n",
      "[XAI-MODEL] Completed hyperparameter tuning.\n",
      "[XAI-MODEL] Trained model with best hyperparameters.\n",
      "[XAI-MODEL] Logged explanation data to data/dataset_config/titanic_config.json\n",
      "[XAI-MODEL] Explanation process completed.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_zero-shot_baseline_batches.jsonl uploaded to batch_inputs/gemini/titanic_zero-shot_baseline_batches.jsonl\n",
      "[ZERO-SHOT] Submitted Job: projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168\n",
      "[ZERO-SHOT] Output base dir: gs://6998final-bucket/batch_outputs/gemini\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/372383421945/locations/us-east4/batchPredictionJobs/1257771758065287168 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ZERO-SHOT] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_zero-shot_1765876674/prediction-model-2025-12-16T09:17:54.753524Z/predictions.jsonl to data/batch_outputs/titanic_zero-shot_baseline_predictions.jsonl.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_zero-shot-cot_baseline_batches.jsonl uploaded to batch_inputs/gemini/titanic_zero-shot-cot_baseline_batches.jsonl\n",
      "[ZERO-SHOT-COT] Submitted Job: projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136\n",
      "[ZERO-SHOT-COT] Output base dir: gs://6998final-bucket/batch_outputs/gemini\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/372383421945/locations/us-east4/batchPredictionJobs/8330674972850651136 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ZERO-SHOT-COT] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_zero-shot-cot_1765877038/prediction-model-2025-12-16T09:23:59.618416Z/predictions.jsonl to data/batch_outputs/titanic_zero-shot-cot_baseline_predictions.jsonl.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[PIPELINE] Baseline metrics computed.\n",
      "Found best number of clusters: k=15 with silhouette score: 0.2761344411376961\n",
      "Chosen 15 diverse examples.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file titanic_reasoning_batches.jsonl: 100%|██████████| 39.9k/39.9k [00:00<00:00, 78.0kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.COMPLETED\n",
      "[REASON GENERATION] Batch completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file titanic_reasoning_predictions.jsonl: 100%|██████████| 107k/107k [00:00<00:00, 23.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Batch outputs downloaded to data/batch_outputs/titanic_reasoning_predictions.jsonl\n",
      "[PIPELINE] Reasoning generation completed.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[OBJECTIVE JUDGE] Submitted batch with id: msgbatch_011AkU6FJxUcHwbCxeRFfDvU\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_011AkU6FJxUcHwbCxeRFfDvU has completed processing.\n",
      "[OBJECTIVE JUDGE] Batch result types: {'succeeded': 15, 'errored': 0, 'expired': 0}\n",
      "[OBJECTIVE JUDGE] Saved evaluations to data/batch_outputs/titanic_objective_judge_evaluations.jsonl\n",
      "[PIPELINE] Objective judge evaluation completed.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_icl_batches.jsonl uploaded to batch_inputs/gemini/titanic_icl_batches.jsonl\n",
      "[ICL CLASSIFIER] Submitted Job: projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120\n",
      "[ICL CLASSIFIER] Output base dir: gs://6998final-bucket/batch_outputs/gemini/titanic_cot_1765878707/\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_QUEUED\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/372383421945/locations/us-east4/batchPredictionJobs/889602488527749120 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ICL CLASSIFIER] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_cot_1765878707/prediction-model-2025-12-16T09:51:48.618452Z/predictions.jsonl to data/batch_outputs/titanic_cot_predictions.jsonl.\n",
      "[PIPELINE] ICL classification with COT completed.\n",
      "[POSTPROCESS] 1 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 179 rows due to NaNs (kept 712 rows).\n",
      "[PIPELINE] Evaluation of COT predictions completed.\n",
      "[PIPELINE] Pipeline run completed.\n",
      "Completed in 2460.54s. Saved to data/results/unmasked/titanic_unmasked_1765876610.json\n",
      "\n",
      "Completed 1 experiments.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MASKED_DATASETS_TO_RUN = [\n",
    "    # \"titanic\",\n",
    "    # \"diabetes\",\n",
    "    # \"loan\",\n",
    "    # \"mushroom\",\n",
    "]\n",
    "UNMASKED_DATASETS_TO_RUN = [\n",
    "    \"titanic\",\n",
    "    # \"diabetes\",\n",
    "    # \"loan\",\n",
    "    # \"mushroom\",\n",
    "]\n",
    "\n",
    "# === Run Experiments ===\n",
    "all_results = []\n",
    "\n",
    "\n",
    "for name in MASKED_DATASETS_TO_RUN:\n",
    "    result = run_pipeline(\n",
    "        datasets_masked[name], output_dir=\"data/results/masked\", masked=True\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "for name in UNMASKED_DATASETS_TO_RUN:\n",
    "    result = run_pipeline(\n",
    "        datasets[name], output_dir=\"data/results/unmasked\", masked=False\n",
    "    )\n",
    "    all_results.append(result)\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} experiments.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
