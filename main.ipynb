{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec8bc181",
   "metadata": {},
   "source": [
    "# Explanability-Driven In-context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada729a",
   "metadata": {},
   "source": [
    "# Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f52666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# modules used for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "# custom preprocessing modules\n",
    "from scripts.preprocess import preprocess_titanic\n",
    "\n",
    "# modules used for model handling\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# modules used for genari pipeline\n",
    "from scripts.pipeline import Pipeline\n",
    "from scripts.configs import Dataset, Model\n",
    "\n",
    "# modules used for env variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")\n",
    "WANDB_PROJECT_NAME = os.getenv(\"WANDB_PROJECT_NAME\")\n",
    "PROJECT_NAME = os.getenv(\"PROJECT_NAME\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388546a1",
   "metadata": {},
   "source": [
    "## Default Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bf4712",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanable_model = XGBClassifier()\n",
    "tune_config_file = \"data/tune_config/xgb.json\"\n",
    "reasoning_gen_model = Model(\n",
    "    provider=\"together\",\n",
    "    name = \"deepseek-ai/DeepSeek-R1\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=4096\n",
    ")\n",
    "objective_judge_model = Model(\n",
    "    provider=\"anthropic\",\n",
    "    name=\"claude-haiku-4-5\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=4096\n",
    ")\n",
    "cot_model = Model(\n",
    "    provider=\"google\",\n",
    "    name=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5695d",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2890b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    name=\"titanic\",\n",
    "    path=\"data/datasets/titanic_small.csv\",\n",
    "    config_file_path=\"data/dataset_config/titanic_config.json\",\n",
    "    shap_vals_path=\"data/shap_values/titanic_shap.csv\",\n",
    "    preprocess_fn=preprocess_titanic,\n",
    "    target_col=\"Survived\",\n",
    "    labels={0: \"Did not survive\", 1: \"Survived\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511432a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Pipeline(\n",
    "    dataset=dataset,\n",
    "    explanable_model=explanable_model,\n",
    "    tune_config_file=tune_config_file,\n",
    "    reasoning_gen_model=reasoning_gen_model,\n",
    "    objective_judge_model=objective_judge_model,\n",
    "    cot_model=cot_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce42c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "Create sweep with ID: bngoq9v8\n",
      "Sweep URL: https://wandb.ai/gauravpendharkar/xai-guided-cot/sweeps/bngoq9v8\n",
      "[XAI-MODEL] Completed hyperparameter tuning.\n",
      "[XAI-MODEL] Trained model with best hyperparameters.\n",
      "[XAI-MODEL] Logged explanation data to data/dataset_config/titanic_config.json\n",
      "[XAI-MODEL] Explanation process completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_zero-shot_baseline_batches.jsonl uploaded to batch_inputs/gemini/titanic_zero-shot_baseline_batches.jsonl\n",
      "[ZERO-SHOT] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328\n",
      "[ZERO-SHOT] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT] projects/54181826632/locations/us-east4/batchPredictionJobs/6150843143005667328 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ZERO-SHOT] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_zero-shot_1765155754/prediction-model-2025-12-08T01:02:35.036750Z/predictions.jsonl to data/batch_outputs/titanic_zero-shot_baseline_predictions.jsonl.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_zero-shot-cot_baseline_batches.jsonl uploaded to batch_inputs/gemini/titanic_zero-shot-cot_baseline_batches.jsonl\n",
      "[ZERO-SHOT-COT] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688\n",
      "[ZERO-SHOT-COT] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_QUEUED\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_RUNNING\n",
      "[ZERO-SHOT-COT] projects/54181826632/locations/us-east4/batchPredictionJobs/6378837874141298688 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ZERO-SHOT-COT] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_zero-shot-cot_1765156120/prediction-model-2025-12-08T01:08:41.807168Z/predictions.jsonl to data/batch_outputs/titanic_zero-shot-cot_baseline_predictions.jsonl.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[PIPELINE] Baseline metrics computed.\n",
      "Found best number of clusters: k=7 with silhouette score: 0.28442837590344316\n",
      "Chosen 7 diverse examples.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading file titanic_reasoning_batches.jsonl: 100%|██████████| 18.6k/18.6k [00:00<00:00, 34.5kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.IN_PROGRESS\n",
      "[REASON GENERATION] Current Status: BatchJobStatus.COMPLETED\n",
      "[REASON GENERATION] Batch completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file titanic_reasoning_predictions.jsonl: 100%|██████████| 52.0k/52.0k [00:00<00:00, 26.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REASON GENERATION] Batch outputs downloaded to data/batch_outputs/titanic_reasoning_predictions.jsonl\n",
      "[PIPELINE] Reasoning generation completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[OBJECTIVE JUDGE] Submitted batch with id: msgbatch_019CHTtjiLir3HpUDR3oUE8M\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_019CHTtjiLir3HpUDR3oUE8M is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_019CHTtjiLir3HpUDR3oUE8M is still processing...\n",
      "[OBJECTIVE JUDGE] Batch msgbatch_019CHTtjiLir3HpUDR3oUE8M has completed processing.\n",
      "[OBJECTIVE JUDGE] Batch result types: {'succeeded': 7, 'errored': 0, 'expired': 0}\n",
      "[OBJECTIVE JUDGE] Saved evaluations to data/batch_outputs/titanic_objective_judge_evaluations.jsonl\n",
      "[PIPELINE] Objective judge evaluation completed.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[GCS CLIENT] File data/batches/titanic_icl_batches.jsonl uploaded to batch_inputs/gemini/titanic_icl_batches.jsonl\n",
      "[ICL CLASSIFIER] Submitted Job: projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424\n",
      "[ICL CLASSIFIER] Output base dir: gs://xai_guided_cot_bucket/batch_outputs/gemini/titanic_cot_1765156796/\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_QUEUED\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_RUNNING\n",
      "[ICL CLASSIFIER] projects/54181826632/locations/us-east4/batchPredictionJobs/7239025402969063424 state: JobState.JOB_STATE_SUCCEEDED\n",
      "[ICL CLASSIFIER] Final state: JobState.JOB_STATE_SUCCEEDED\n",
      "[GCS CLIENT] Downloaded batch_outputs/gemini/titanic_cot_1765156796/prediction-model-2025-12-08T01:19:57.482547Z/predictions.jsonl to data/batch_outputs/titanic_cot_predictions.jsonl.\n",
      "[PIPELINE] ICL classification with COT completed.\n",
      "[POSTPROCESS] 0 requests were interrupted due to token limit and are ignored for evaluation.\n",
      "[Titanic] Dropped 39 rows due to NaNs (kept 161 rows).\n",
      "[PIPELINE] Evaluation of COT predictions completed.\n",
      "[PIPELINE] Pipeline run completed.\n"
     ]
    }
   ],
   "source": [
    "llm.run(baseline=True, objective_judge=True, cot_ablation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
